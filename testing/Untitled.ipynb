{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = []\n",
    "with open('Software_5.json') as f:\n",
    "  for line in f:\n",
    "    rev.append(json.loads(line))\n",
    "\n",
    "#main dataframe\n",
    "test = pd.DataFrame(columns=['Summary and Review', 'Rating'])\n",
    "\n",
    "#adding data into dataframe \n",
    "for i in range(len(rev)):\n",
    "    if('overall' in rev[i] and 'reviewText' in rev[i] and 'summary' in rev[i]):\n",
    "        test.loc[i] = [rev[i]['summary'] + ' ' + rev[i]['reviewText'], rev[i]['overall']]\n",
    "#function to turn ratings into two class\n",
    "def rate(x):\n",
    "    if(x>3):\n",
    "        return 1\n",
    "    elif(x<3):\n",
    "        return 0\n",
    "    else:\n",
    "        return random.randint(0,1)\n",
    "test['Rating'] = test['Rating'].apply(rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []\n",
    "for i in range(len(test)):\n",
    "    sample.append(''.join(test.iloc[i]['Summary and Review'].split('\\n')).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readwords( filename ):\n",
    "    f = open(filename, encoding = \"ISO-8859-1\")\n",
    "    words = [ line.rstrip() for line in f.readlines()]\n",
    "    f.close()\n",
    "    return words\n",
    "    \n",
    "\n",
    "positive = readwords('positive-words.txt')\n",
    "negative = readwords('negative-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = negative[36:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = positive[36:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection(sample[0],positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Summary and Review\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection(sample[0],negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []\n",
    "for i in range(len(test)):\n",
    "    temp = ''.join(test.iloc[i]['Summary and Review'].split('\\n'))\n",
    "#     x = re.sub(r'[0-9]+|_', '', x)\n",
    "    temp = \" . \".join(temp.split('.')).split()\n",
    "    sample.append(temp)\n",
    "x = []\n",
    "from nltk.sentiment.util import mark_negation\n",
    "for i in range(len(sample)):\n",
    "    x.append(mark_negation(sample[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    x[i] = list(value for value in x[i] if value != \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another=[]\n",
    "# p=0\n",
    "# n=0\n",
    "# for doc in x:\n",
    "#     for word in doc:\n",
    "#         if word in positive:\n",
    "#             p+=1\n",
    "#         elif word in negative:\n",
    "#             n+=1\n",
    "#         elif \"NEG\" in word:\n",
    "#             n+=1\n",
    "#     another.append([p, n, len(doc)])\n",
    "#     n=0, p=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_neg_temp = 0\n",
    "count_neg = []\n",
    "x_copy = x.copy()\n",
    "for i in range(len(x_copy)):\n",
    "    for j in range(len(x_copy[i])):\n",
    "        if \"NEG\" in x_copy[i][j]:\n",
    "            count_neg_temp+=1\n",
    "    count_neg.append(count_neg_temp)\n",
    "    count_neg_temp = 0\n",
    "count_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = []\n",
    "counter = 0\n",
    "for doc in sample:\n",
    "    tNeg = len(intersection(doc,negative))\n",
    "    tPos = len(intersection(doc,positive))\n",
    "    lenDoc = len(doc)\n",
    "    simple.append([tPos,tNeg + count_neg[counter],lenDoc])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(simple, test['Rating'], test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "print(str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "predicted = BNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "print(str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "predicted = LG.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ([np.array(simple), np.array(test['Rating'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import metrics\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    score = clf.score(X_test, Y_test)\n",
    "    accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "    print('Accuracy with ' + name + ': ' + str(accuracy_score*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel=\"linear\", C=0.025)\n",
    "svc.fit(X_train, Y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "predicted = svc.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "print(str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "3faf17de123bcda6419610acc6b34a663870666c5ab768fcfaaae3c23ee7f58d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}